## https://pytorch.org/docs/1.8.1/data.html?highlight=dataloader#torch.utils.data.DataLoader

train:
  in2mem: False
  itersepo: ~ ## assigned in runtime @ get_trainloader 

  bs: 32 #???
  #shuffle: True ## theres a rndsampler
  nworkers: ${dyn_nworkers:0}  ## 0 -> cpu//4 ;; -1 -> all cores
  pinmem: True
  droplast: True ## many functions relly on bs, so if true irregular len of batch ...
  #pftch_fctr: 2
  prstwrk: True

  ## runs the dl twice after returns 
  dryrun: False

## both validate and test reuse this values
## as the dataloader is created from same point
test:
  in2mem: False
  itersepo: ~ ## assigned in runtime @ get_testloader 

  bs: 1
  shuffle: False
  nworkers: ${dyn_nworkers:0}  ## 0 -> cpu//4 ;; -1 -> all cores  
  pinmem: True
  droplast: False
  #pftch_fctr: 2
  prstwrk: True

  ## runs the dl twice after returns 
  dryrun: False